{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_TARGETS = [f\"{_} wk ahead inc death\" for _ in range(1, 5)] + \\\n",
    "                [f\"{_} wk ahead cum death\" for _ in range(1, 5)] + \\\n",
    "                [f\"{_} wk ahead inc case\" for _ in range(1, 5)]\n",
    "\n",
    "MODELS_TO_EXCLUDE = ['COVIDhub-ensemble', 'COVIDhub-trained_ensemble', \n",
    "                     'CU-nochange', 'CU-scenario_high', 'CU-scenario_low', 'CU-scenario_mid']\n",
    "\n",
    "LOCATIONS_TO_EXCLUDE = [\"11\", \"60\", \"66\", \"69\", \"72\", \"74\", \"78\"]\n",
    "\n",
    "# DC,11,District of Columbia\n",
    "# AS,60,American Samoa\n",
    "# GU,66,Guam\n",
    "# MP,69,Northern Mariana Islands\n",
    "# PR,72,Puerto Rico\n",
    "# UM,74,U.S. Minor Outlying Islands\n",
    "# VI,78,Virgin Islands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_monday(date):\n",
    "    return pd.date_range(start=date, end=date + pd.offsets.Day(6), freq='W-MON')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_filepaths_and_dates(models_to_exclude, online=False):\n",
    "    if online:\n",
    "        url = \"https://api.github.com/repos/reichlab/covid19-forecast-hub/git/trees/master?recursive=1\"\n",
    "        r = requests.get(url)\n",
    "        res = r.json()\n",
    "\n",
    "        files = [file[\"path\"] for file in res[\"tree\"] if (file[\"path\"].startswith('data-processed/') and file[\"path\"].endswith('.csv'))]\n",
    "    else:\n",
    "        files = glob.glob('../covid19-forecast-hub/data-processed/**/*.csv', recursive=True)\n",
    "        files = [f.replace('../covid19-forecast-hub/', '').replace('\\\\', '/') for f in files]\n",
    "\n",
    "    df_files = pd.DataFrame({'filename':files})\n",
    "\n",
    "    df_files['model'] = df_files.filename.apply(lambda f: f.split('/')[1])\n",
    "\n",
    "    df_files['forecast_date'] = df_files.filename.apply(lambda f: f.split('/')[2][:10])\n",
    "    df_files.forecast_date = pd.to_datetime(df_files.forecast_date)\n",
    "\n",
    "    df_files['timezero'] = df_files.forecast_date.apply(next_monday)\n",
    "\n",
    "    df_files = df_files[~df_files.model.isin(models_to_exclude)]\n",
    "    \n",
    "    df_files.sort_values('filename', inplace=True, ignore_index=True)\n",
    "\n",
    "    return df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_files = get_all_filepaths_and_dates(MODELS_TO_EXCLUDE, online=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_submissions(df, locations_to_exclude, train_set = True):\n",
    "    # only consider US + 50 states\n",
    "    df = df[df.location.str.len() == 2]\n",
    "    df = df[~df.location.isin(locations_to_exclude)]\n",
    "\n",
    "    df = df[df.type == 'quantile']\n",
    "    df.dropna(subset = ['value'], inplace = True)\n",
    "    \n",
    "    if train_set:\n",
    "        # how many forecasts for each target/model/location? should be 4 for every location\n",
    "        df['no_forecasts'] = df.groupby(['target', 'model', 'location'])['target_end_date'].transform('nunique')\n",
    "        df = df[df.groupby(['target', 'model'])['no_forecasts'].transform('min') == 4].drop(columns='no_forecasts').reset_index(drop=True)\n",
    "\n",
    "    df['no_quantiles'] = df.groupby(['model', 'target', 'target_end_date', 'location'])['quantile'].transform('nunique')\n",
    "    df['no_quantiles'] = df.groupby(['target', 'model'])['no_quantiles'].transform('min')\n",
    "\n",
    "    df = df[(df.no_quantiles == 23) | \n",
    "            (df.target.str.contains('inc case') & (df.no_quantiles == 7))].drop(columns='no_quantiles').reset_index(drop=True)\n",
    "\n",
    "    # ensure that for all targets each model provides forecasts for all locations\n",
    "    df = df[df.groupby(['target', 'model'])['location'].transform('nunique') == 51]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(files, test_date, valid_targets, locations_to_exclude, online=False):\n",
    "    test_date = pd.to_datetime(test_date)\n",
    "    df_test_files = files[files.timezero == test_date]\n",
    "    \n",
    "    if online:\n",
    "        base_path = 'https://github.com/reichlab/covid19-forecast-hub/raw/master/'\n",
    "    else:\n",
    "        base_path = '../covid19-forecast-hub/'\n",
    "    \n",
    "    dfs = []\n",
    "    for _, row in tqdm(df_test_files.iterrows(), total=df_test_files.shape[0], desc = 'Load test data'):\n",
    "        df_temp = pd.read_csv(base_path + row['filename'],\n",
    "                              dtype = {'target': str, 'location': str, 'type': str, 'quantile': float, 'value': float}, \n",
    "                              parse_dates = ['forecast_date', 'target_end_date'])\n",
    "        df_temp = df_temp[df_temp.target.isin(VALID_TARGETS)]\n",
    "        df_temp['model'] = row['model']\n",
    "        dfs.append(df_temp)\n",
    "    df_test = pd.concat(dfs)\n",
    "    \n",
    "    df_test = validate_submissions(df_test, locations_to_exclude, train_set = False)\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ab4eeb1eb240f29cd51f310ef8f15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load test data:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_test = load_test_data(df_files, '2021-06-14', VALID_TARGETS, LOCATIONS_TO_EXCLUDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d - 4 weeks - (horizon - 1) --> end: d - horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_dates_by_horizon(test_date, window_size = 4):\n",
    "    # assigns to each horizon the corresponding training forecast dates for the test date\n",
    "    forecast_dates_by_horizon = {}\n",
    "    for horizon in range(1, 5):\n",
    "        forecast_dates_by_horizon[horizon] = [test_date - pd.Timedelta(weeks = window_size) - pd.Timedelta(weeks = (horizon - 1)), \n",
    "                     test_date - pd.Timedelta(weeks = horizon)]\n",
    "    return forecast_dates_by_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_horizons(forecast_date, forecast_dates_by_horizon):\n",
    "    relevant_horizons = []\n",
    "    for horizon in range(1, 5):\n",
    "        if((forecast_date >= forecast_dates_by_horizon[horizon][0]) & (forecast_date <= forecast_dates_by_horizon[horizon][1])):\n",
    "            relevant_horizons.append(horizon)\n",
    "    return relevant_horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(files, test_date, valid_targets, locations_to_exclude, window_size = 4, online = False):\n",
    "    test_date = pd.to_datetime(test_date)\n",
    "    lower_bound = test_date - pd.Timedelta(weeks = window_size) - pd.Timedelta(weeks=(window_size - 1))\n",
    "    df_train_files = files[(files.timezero >= lower_bound) & (df_files.timezero < test_date)].copy()\n",
    "    \n",
    "    forecast_dates_by_horizon = get_forecast_dates_by_horizon(test_date, window_size)\n",
    "    df_train_files['horizons'] = df_train_files.timezero.apply(get_relevant_horizons, \n",
    "                                                               forecast_dates_by_horizon=forecast_dates_by_horizon)\n",
    "    \n",
    "    if online:\n",
    "        base_path = 'https://github.com/reichlab/covid19-forecast-hub/raw/master/'\n",
    "    else:\n",
    "        base_path = '../covid19-forecast-hub/'\n",
    "    \n",
    "    dfs = []\n",
    "    for _, row in tqdm(df_train_files.iterrows(), total=df_train_files.shape[0], desc = 'Load train data'):\n",
    "        relevant_targets = [f\"{_} wk ahead inc death\" for _ in row['horizons']] + \\\n",
    "                           [f\"{_} wk ahead cum death\" for _ in row['horizons']] + \\\n",
    "                           [f\"{_} wk ahead inc case\" for _ in row['horizons']]\n",
    "        df_temp = pd.read_csv(base_path + row['filename'],\n",
    "                              dtype = {'target': str, 'location': str, 'type': str, 'quantile': float, 'value': float}, \n",
    "                              parse_dates = ['forecast_date', 'target_end_date'])\n",
    "        df_temp = df_temp[df_temp.target.isin(relevant_targets)]\n",
    "        df_temp['model'] = row['model']\n",
    "        dfs.append(df_temp)\n",
    "    df_train = pd.concat(dfs)\n",
    "    \n",
    "    df_train = validate_submissions(df_train, locations_to_exclude, train_set = True)\n",
    "    \n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25ebbca036e4593a6462f2e3079f314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load train data:   0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_train = load_train_data(df_files, '2021-06-14', VALID_TARGETS, LOCATIONS_TO_EXCLUDE, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test(test_date, valid_targets, locations_to_exclude, models_to_exclude, window_size = 4, online = False):\n",
    "    files = get_all_filepaths_and_dates(models_to_exclude)\n",
    "    \n",
    "    df_test = load_test_data(files, test_date, valid_targets, locations_to_exclude, online)\n",
    "    df_train = load_train_data(files, test_date, valid_targets, locations_to_exclude, window_size, online)\n",
    "    \n",
    "    # dicts of the models available for each target\n",
    "    available_models_test  = dict(df_test.groupby(['target'])['model'].unique())\n",
    "    available_models_train = dict(df_train.groupby(['target'])['model'].unique())\n",
    "    \n",
    "    # ensure models are available in both train and test set\n",
    "    df_train = df_train[df_train.apply(lambda x: x.model in (available_models_test[x.target]), axis=1)]\n",
    "    df_test  = df_test[df_test.apply(lambda x: x.model in (available_models_train[x.target]), axis=1)]\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load test data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f85f0e60c5741d98b6e9f95a80823fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c7a6ed36bc4f06a39d317d5c1b6507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_train, df_test = load_train_test('2021-06-14', VALID_TARGETS, LOCATIONS_TO_EXCLUDE, MODELS_TO_EXCLUDE, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_train_test_sets(test_dates, valid_targets, locations_to_exclude, models_to_exclude, window_size = 4, online = False):\n",
    "    for test_date in test_dates:\n",
    "        print(test_date)\n",
    "        df_train, df_test = load_train_test(test_date, valid_targets, locations_to_exclude, models_to_exclude, \n",
    "                                            window_size, online)\n",
    "        df_train.to_csv(f'data/{test_date}_df_train.csv', index=False)\n",
    "        df_test.to_csv(f'data/{test_date}_df_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-07\n",
      "Load test data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277e5765e8e743e19fc27eff3a51b2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1759c794788042c8aab3985ba8b759d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14\n",
      "Load test data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40dc3a96c7e4d3cbfc4746e7a400a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70082960a8a744a4bbb817d12a97e459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_train_test_sets(['2021-06-07', '2021-06-14'], VALID_TARGETS, LOCATIONS_TO_EXCLUDE, MODELS_TO_EXCLUDE, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
